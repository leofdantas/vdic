% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/03_builder.R
\name{vectionary_builder}
\alias{vectionary_builder}
\title{Build a vec-tionary from dictionary and embeddings}
\usage{
vectionary_builder(
  dictionary,
  embeddings,
  modality = "text",
  language = "en",
  dimensions = NULL,
  binary_word = TRUE,
  method = "ridge",
  l1_ratio = 0.5,
  lambda = "gcv",
  min_validity = 0.75,
  expand_vocab = NULL,
  spellcheck = TRUE,
  expand_positive = TRUE,
  expand_stem = FALSE,
  remove_stopwords = TRUE,
  save_path = getwd(),
  verbose = TRUE,
  seed = NULL
)
}
\arguments{
\item{dictionary}{Either a data frame or a character vector of words.
- **Data frame**: Must have a 'word' column plus one or more dimension columns
  (e.g., 'care', 'fairness', 'sentiment'). ALL words are used to learn axes.
- **Character vector**: A simple list of seed words. Creates a binary dictionary
  where all words score 1 on each dimension specified (or "score" if dimensions=NULL).}

\item{embeddings}{Path to word embeddings file. Can be FastText .vec format,
word2vec .bin format, or GloVe .txt format.}

\item{modality}{Embedding modality (default: "text"). Options:
\itemize{
  \item "text": Use traditional word embeddings (FastText, word2vec, GloVe)
  \item "multimodal": Use SigLIP multi-modal embeddings. Dictionary words are
    encoded in the shared text-image embedding space (512-dim), creating axes
    that can analyze both text and images. Requires embeddings parameter to be
    "siglip" or a path to pre-computed SigLIP embeddings.
}}

\item{language}{Language code for stopwords and spell checking (default: "en").
Any language supported by the wooorm/dictionaries repository can be used
(e.g., "en", "pt", "es", "fr", "de", "it", "nl", "ru"). When spellcheck = TRUE,
the hunspell dictionary for this language is automatically downloaded and cached
in vdic_data/dictionaries/ (in the working directory) if not already available. Stopword lists are built-in
for "en", "pt", and "es"; other languages skip stopword removal unless a custom
list is provided via \code{remove_stopwords}.
See https://github.com/wooorm/dictionaries for all available languages.}

\item{dimensions}{Character vector of dimension names to build axes for.
If NULL (default), uses all columns in dictionary except 'word'.}

\item{binary_word}{Logical. If TRUE (default), treats dictionary as binary:
converts all non-zero values to 1 and blanks/NAs/NaNs/nulls to 0.
If FALSE, uses continuous scores as provided in dictionary.}

\item{method}{Regularization method for axis learning (default: "ridge"). Options:
\itemize{
  \item "ridge": Ridge regression (L2 penalty) - smooth, dense axes
  \item "elastic_net": Elastic net (L1 + L2 penalty) - balanced sparsity
  \item "lasso": LASSO (L1 penalty) - maximum sparsity, sets l1_ratio=1
  \item "duan": Duan et al. (2025) method - constrained nonlinear optimization
    with unit norm constraint, no regularization. Replicates the vMFD approach.
}}

\item{l1_ratio}{Elastic net mixing parameter (default: 0.5). Only used when
method="elastic_net". Value between 0 (pure ridge) and 1 (pure lasso).
Controls sparsity of learned axes. Higher values produce sparser axes with
more zero coefficients. Ignored for method="ridge" or "lasso".}

\item{lambda}{Regularization parameter. Can be:
\itemize{
  \item "gcv" (default): Use Generalized Cross-Validation (Golub et al., 1979) to select
    optimal lambda. Only works with method="ridge".
  \item Numeric value (e.g., 0.5): Use this specific lambda
  \item Numeric vector (e.g., c(0.01, 0.1, 1)): Test these values and select optimal
}
Higher lambda creates more regularized axes. When multiple values are provided,
the optimal lambda is selected based on validity (R² or AUC) and axis differentiation.}

\item{min_validity}{Minimum validity threshold (default: 0.75 = 75%).
Only used when lambda is a numeric vector (ignored for "gcv").
For continuous dictionaries: R² between scores and projections.
For binary dictionaries: AUC (0.5 = random, 1.0 = perfect separation).
Lambda candidates below this threshold are rejected unless none pass.}

\item{expand_vocab}{Integer or NULL (default). If set, expands the training
dictionary before learning axes. First learns preliminary axes from seed words,
then finds the top-N words with highest projections onto those axes. These
words are added to the dictionary (with their projection as the score), and
final axes are learned from the expanded dictionary. This improves axis quality
by increasing the training signal.}

\item{spellcheck}{Logical (default: TRUE). If TRUE, filters the embedding
vocabulary using hunspell spell checking (with the dictionary matching
the \code{language} parameter) before projection. This removes non-words,
typos, symbols, and web artifacts commonly found in embeddings trained on
Common Crawl data. The spell check is applied first, so expand_stem and
expand_vocab will only find valid dictionary words. Requires the hunspell
and data.table packages.}

\item{expand_positive}{Logical (default: TRUE). If TRUE and expand_vocab is set,
only adds words with positive projections onto the preliminary axes. Recommended
for binary dictionaries where negative projections are not semantically meaningful.}

\item{expand_stem}{Logical (default: FALSE). If TRUE, expands dictionary words
ending with * (e.g., "abandon*") to all matching words found in the embeddings
(e.g., "abandon", "abandoned", "abandoning", "abandonment"). The expanded words
inherit the same scores as the stem pattern. This is useful for dictionaries
built with word stems or regex-style patterns.}

\item{remove_stopwords}{Logical (default: TRUE). If TRUE, removes stopwords
(based on the \code{language} parameter) from the embedding space so that
the vectionary is not projected to these words. Useful for simplicity and
interpretability, as stopwords are the most common words and their scores
can bias the output of vectionary_analyze(). Can also be a character vector
of custom stopwords.}

\item{save_path}{Where to save the vectionary. Can be:
\itemize{
  \item A filename ending in `.rds` (e.g., "my_vect.rds") - saves to working directory
  \item A full path with filename (e.g., "~/data/my_vect.rds") - saves to that path
  \item A directory path (e.g., getwd()) - saves as "vectionary.rds" in that directory
  \item NULL - does not save
}
Default: current working directory (saves as "vectionary.rds").}

\item{verbose}{Logical (default: TRUE). If TRUE, prints step-by-step progress
messages: dictionary summary, embedding loading, lambda selection, axis
learning, vocabulary expansion, projection, and save confirmation.}

\item{seed}{Integer seed for reproducibility. Controls random operations
(Duan method initialization, AUC validation sampling). If NULL (default),
a random seed is generated and stored in \code{metadata$seed} so the build
can be reproduced later.}
}
\value{
A vectionary object containing:
  \itemize{
    \item axes: Named list of dimension vectors in embedding space
    \item word_projections: Pre-computed projections for ALL words in the embeddings
    \item dimensions: Names of dimensions
    \item metadata: Build information (method, embedding source, vocab_size, etc.)
  }
}
\description{
Builds a vector-based dictionary (vec-tionary) by learning axes in embedding
space from seed words in the input dictionary, then projects ALL words from
the embeddings onto these axes. This replicates the Duan et al. (2025) approach
where the vectionary can score any word in the full embedding vocabulary.
The resulting file size depends on the embedding vocabulary (typically ~3 MB
as a compressed RDS).
}
\examples{
\dontrun{
# Portuguese moral foundations example
dictionary <- data.frame(
  word = c("proteger", "cuidar", "ajudar", "machucar", "prejudicar"),
  care = c(0.9, 0.8, 0.7, -0.8, -0.7),
  fairness = c(0.1, 0.2, 0.3, 0.0, 0.1)
)

# Default: GCV (Generalized Cross-Validation) for optimal lambda
my_vect <- vectionary_builder(
  dictionary = dictionary,
  embeddings = "vdic_data/cc.pt.300.vec",
  dimensions = c("care", "fairness")
)

# Specify lambda manually (single value)
my_vect <- vectionary_builder(
  dictionary = dictionary,
  embeddings = "vdic_data/cc.pt.300.vec",
  dimensions = c("care", "fairness"),
  lambda = 0.5
)

# Select from custom range
my_vect <- vectionary_builder(
  dictionary = dictionary,
  embeddings = "vdic_data/cc.pt.300.vec",
  dimensions = c("care", "fairness"),
  lambda = c(0.01, 0.1, 0.5, 1)
)

# Use elastic net for sparse axes
my_vect_sparse <- vectionary_builder(
  dictionary = dictionary,
  embeddings = "vdic_data/cc.pt.300.vec",
  dimensions = c("care", "fairness"),
  method = "elastic_net",
  l1_ratio = 0.8,  # More LASSO-like (sparser)
  lambda = c(0.1, 0.5, 1)
)

# Use the vec-tionary
my_vect$mean("Devemos proteger as pessoas vulneráveis")

# Vectionary is automatically saved to save_path (default: working directory)
# To skip saving, use save_path = NULL

# Simple word list (character vector) - creates binary dictionary
care_words <- c("protect", "care", "help", "safe", "harm", "hurt")
care_vect <- vectionary_builder(
  dictionary = care_words,
  embeddings = "vdic_data/cc.en.300.vec",
  dimensions = "care"
)
}
}
