% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/01_package.R
\docType{package}
\name{vdic-package}
\alias{vdic}
\alias{vdic-package}
\title{vdic: Build and Use Vec-tionaries for Text and Image Analysis}
\description{
A framework for building and using vector-based dictionaries (vec-tionaries)
for text and image analysis. Provide seed words scored on dimensions of
interest (e.g., moral foundations, sentiment, populism), and vdic learns
semantic axes in embedding space that can score \emph{any} word or image.

Text-based analysis uses R-only code and traditional word embeddings
(FastText, word2vec, GloVe). Image analysis uses SigLIP multi-modal
embeddings (\code{google/siglip-so400m-patch14-384}), which map both text
and images into the same 1152-dimensional space â€” allowing text dictionaries
to score image content directly. Image support requires Python with the
\code{transformers}, \code{torch}, and \code{Pillow} libraries installed and
accessible via the \code{reticulate} package.
}
\details{
\strong{Design philosophy.}
The package separates code from data.
It ships R code only.
Users download word embeddings once (~1--7 GB), build lightweight
vec-tionaries (~3 MB), and share them freely without the original
embeddings.
}
\section{Core Workflow}{


\strong{Step 1. Download embeddings} (one-time setup):
\preformatted{
download_embeddings(language = "pt", model = "fasttext")
}

\strong{Step 2. Build a vec-tionary} from a seed dictionary + embeddings:
\preformatted{
dictionary <- data.frame(
  word = c("proteger", "cuidar", "machucar"),
  care = c(0.9, 0.8, -0.8)
)
my_vect <- vectionary_builder(
  dictionary = dictionary,
  embeddings = "vdic_data/cc.pt.300.vec"
)
}

\strong{Step 3a. Analyze text} using the vec-tionary:
\preformatted{
my_vect$mean("Devemos proteger as pessoas vulneraveis")
vectionary_analyze(my_vect, texts, metric = "mean")
}

\strong{Step 3b. Analyze images} (multi-modal; requires Python):
\preformatted{
# Build with SigLIP instead of word embeddings
mm_vect <- vectionary_builder(
  dictionary = dictionary,
  embeddings = "siglip",
  modality   = "multimodal"
)
analyze_image(mm_vect, images = c("photo1.jpg", "photo2.jpg"))
}

\strong{Step 4. Save and share} (no embeddings needed):
\preformatted{
saveRDS(my_vect, "my_vectionary.rds")  # ~3 MB
loaded <- readRDS("my_vectionary.rds")
}
}

\section{Exported Functions}{


\describe{
  \item{[download_embeddings()]}{Download pre-trained word embeddings
    (FastText for 157 languages, word2vec for English/Portuguese, GloVe for
    English).}
  \item{[vectionary_builder()]}{Build a vec-tionary by learning semantic
    axes from a seed dictionary and projecting the full embedding vocabulary
    onto those axes. Supports ridge, elastic net, LASSO, and Duan et al.
    (2025) methods.}
  \item{[vectionary_analyze()]}{Score one or more documents against a
    vec-tionary. Returns per-dimension scores using a chosen aggregation
    metric (mean, MSE, SD, SE, top-10, top-20, or all). Optionally
    classifies documents via a one-tailed t-test.}
  \item{[vectionary_diagnose()]}{Print a diagnostic report showing the
    top-scoring words per dimension and whether seed words rank near the
    top. Useful for verifying axis quality.}
  \item{[analyze_image()]}{Score image files against a multi-modal
    vec-tionary (built with \code{modality = "multimodal"}). Returns a
    data frame with one row per image and one column per dimension.
    Requires Python with \code{transformers}, \code{torch}, and
    \code{Pillow} via \code{reticulate}.}
}
}

\section{Dollar-Sign Methods}{


Vec-tionary objects support the following via the \code{$} operator:

\describe{
  \item{\code{$mean(text)}}{Arithmetic mean of word projections.}
  \item{\code{$mse(text)}}{Mean squared error (emphasizes high-magnitude
    projections).}
  \item{\code{$sd(text)}}{Standard deviation of projections within a
    document.}
  \item{\code{$se(text)}}{Standard error of the mean.}
  \item{\code{$top_10(text)}}{Mean of the 10 highest projections.}
  \item{\code{$top_20(text)}}{Mean of the 20 highest projections.}
  \item{\code{$metrics(text)}}{All six metrics at once.}
  \item{\code{$diagnose(n, dimension)}}{Diagnostic report (see
    [vectionary_diagnose()]).}
}

All methods accept a single string or a character vector and return
a named list (one element per dimension). Use \code{as.data.frame()}
to convert to a data frame.
}

\section{Mathematical Background}{


\strong{Axis learning.}
Given seed-word embeddings \eqn{W \in \mathbb{R}^{n \times d}} (unit-normalized)
and dictionary scores \eqn{y \in \mathbb{R}^n}, the package learns
a semantic axis \eqn{m \in \mathbb{R}^d}:

\itemize{
  \item \strong{Ridge} (default):
    \eqn{m = (W^\top W + \lambda I)^{-1} W^\top y}, where \eqn{\lambda}
    is selected automatically via GCV (Golub et al., 1979).
  \item \strong{Elastic net / LASSO}: Solved via \code{glmnet::glmnet()}
    with \eqn{\lambda} chosen by cross-validation.
  \item \strong{Duan et al. (2025)}:
    \eqn{\min_m \sum_i (w_i^\top m - s_i)^2} subject to
    \eqn{\|m\| = 1}. No regularization; unit-norm constraint.
}

\strong{Projection.}
All word embeddings are normalized to unit Euclidean norm before axis
learning and projection.
A word's score is the dot product \eqn{\hat{w}_j^\top m}, which equals
\eqn{\|m\| \cos\theta_j} -- cosine similarity scaled by the axis
magnitude.

\strong{Topic classification.}
When \code{alpha} is passed to [vectionary_analyze()], documents are
classified via a one-tailed t-test.
A document is flagged when its score exceeds
\eqn{\bar{x} + t_{1-\alpha,\, n-1} \cdot s}, where \eqn{\bar{x}} and
\eqn{s} are the corpus mean and sample standard deviation.
}

\section{Build Pipeline}{


[vectionary_builder()] runs a 7-step pipeline:
\enumerate{
  \item \strong{Filter embedding vocabulary} -- Remove non-alphabetic tokens,
    stopwords, and misspelled words (hunspell).
  \item \strong{Expand stem patterns} -- Match patterns like \code{"abandon*"}
    to all inflected forms in the filtered embeddings.
  \item \strong{Select lambda} -- Choose the regularization parameter via
    GCV (ridge), cross-validation (elastic net / LASSO), or grid search.
  \item \strong{Learn axes} -- Solve a regression problem per dimension,
    mapping seed-word embeddings to dictionary scores.
  \item \strong{Expand vocabulary} (optional) -- Find top-N words with
    highest projections and add them to the dictionary.
  \item \strong{Rebuild} (if expanded) -- Re-learn axes with the enlarged
    dictionary.
  \item \strong{Project full vocabulary} -- Dot-product every word in the
    filtered embeddings with the learned axes. Package into a
    \code{Vec-tionary} object and save.
}
}

\section{Dependencies}{


\strong{Required}: \code{R.utils}, \code{cli}, \code{MASS}, \code{glmnet},
\code{data.table}.

\strong{Optional}:
\itemize{
  \item \code{hunspell}: Required for \code{spellcheck = TRUE} (default)
    in [vectionary_builder()]. Filters embeddings to exclude non-words,
    symbols, and web artifacts.
  \item \code{alabama}: Required for \code{method = "duan"} (constrained
    optimization).
  \item \code{reticulate} (>= 1.30): Required for \code{modality =
    "multimodal"} in [vectionary_builder()] and for [analyze_image()].
    Also requires a Python environment with \code{transformers},
    \code{torch}, \code{Pillow}, and \code{sentencepiece}. Set up once with:
    \code{reticulate::py_install(c("transformers", "torch", "Pillow", "sentencepiece"))}.
    Text-based analysis does not require Python or \code{reticulate}.
}
}

\references{
Duan, Z., Shao, A., Hu, Y., Lee, H., Liao, X., Suh, Y. J., Kim, J.,
et al. (2025). Constructing Vec-Tionaries to Extract Message Features
from Texts: A Case Study of Moral Content. \emph{Political Analysis},
1--21. \doi{10.1017/pan.2025.6}.

Hopp, F. R., Fisher, J. T., Cornell, D., Huskey, R., & Weber, R. (2021).
The extended Moral Foundations Dictionary (eMFD): Development and
applications of a crowd-sourced approach to extracting moral intuitions
from text. \emph{Behavior Research Methods}, 53(1), 232--246.
}
\seealso{
\itemize{
  \item [download_embeddings()] for obtaining word embeddings
  \item [vectionary_builder()] for building custom vec-tionaries
  \item [vectionary_analyze()] for scoring text
  \item [vectionary_diagnose()] for diagnostic reports
}
}
\author{
Leonardo Dantas
}
\keyword{internal}
