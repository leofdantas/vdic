% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/04_vectionary.R
\name{.tokenize_and_match}
\alias{.tokenize_and_match}
\title{Tokenize, match, and group tokens by document (internal)}
\usage{
.tokenize_and_match(texts, word_projections, dimensions)
}
\arguments{
\item{texts}{Character vector of documents}

\item{word_projections}{Data frame with 'word' column and dimension columns}

\item{dimensions}{Character vector of dimension names}
}
\value{
List with:
  - `matched_scores`: matrix (n_matched_tokens x n_dims) of projection scores
    for every matched token across all documents
  - `doc_groups`: named list where each element is a vector of row indices
    into matched_scores belonging to that document. Names are document indices
    as character strings (e.g., "1", "5", "42"). Documents with zero matches
    are absent from this list.
  - `n_docs`: total number of input documents
  - `n_dims`: number of dimensions
}
\description{
Shared pipeline for batch processing of multiple documents. The key
optimization is doing a single match() call across all tokens from all
documents at once, rather than one match() per document.

The pipeline:
  1. Tokenize each document, tracking how many tokens each produces
  2. Concatenate all tokens into one long vector
  3. Run a single match() against the vocabulary
  4. Use split() to group matched token indices back by document
}
\keyword{internal}
